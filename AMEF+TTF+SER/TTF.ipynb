{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "8748446b-6c9e-4abb-8358-2a157a45bed3",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Code from\n",
    "\" GRATIS: Deep Learning Graph Representation\n",
    " with Task-specific Topology and\n",
    " Multi-dimensional Edge Features\"\n",
    "Siyang Song, Yuxin Song, Cheng Luo, Zhiyuan Song, Selim Kuzucu, Xi Jia, Zhijiang Guo, Weicheng Xie,\n",
    " Linlin Shen, and Hatice Gunes\n",
    "Please see https://github.com/SSYSteve/GRATIS\n",
    "\"\"\"\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "import torch.nn.functional as F\n",
    "import math\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "from audtorch.metrics import ConcordanceCC\n",
    "import os\n",
    "from torch.autograd import Variable\n",
    "\n",
    "os.environ['CUDA_VISIBLE_DEVICES']='0'\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "def bn_init(bn):\n",
    "    bn.weight.data.fill_(1)\n",
    "    bn.bias.data.zero_()\n",
    "class CrossAttn(nn.Module):\n",
    "    \"\"\" cross attention Module\"\"\"\n",
    "    def __init__(self, in_channels):\n",
    "        super(CrossAttn, self).__init__()\n",
    "        self.in_channels = in_channels\n",
    "        self.linear_q = nn.Linear(in_channels, in_channels // 2)\n",
    "        self.linear_k = nn.Linear(in_channels, in_channels // 2)\n",
    "        self.linear_v = nn.Linear(in_channels, in_channels)\n",
    "        self.scale = (self.in_channels // 2) ** -0.5\n",
    "        self.attend = nn.Softmax(dim=-1)\n",
    "\n",
    "        self.linear_k.weight.data.normal_(0, math.sqrt(2. / (in_channels // 2)))\n",
    "        self.linear_q.weight.data.normal_(0, math.sqrt(2. / (in_channels // 2)))\n",
    "        self.linear_v.weight.data.normal_(0, math.sqrt(2. / in_channels))\n",
    "\n",
    "    def forward(self, y, x):\n",
    "        query = self.linear_q(y)\n",
    "        key = self.linear_k(x)\n",
    "        value = self.linear_v(x)\n",
    "        dots = torch.matmul(query, key.transpose(-2, -1)) * self.scale\n",
    "        attn = self.attend(dots)\n",
    "        out = torch.matmul(attn, value)\n",
    "        return out\n",
    "\n",
    "\n",
    "class GEM(nn.Module):\n",
    "    def __init__(self, in_channels, num_classes):\n",
    "        super(GEM, self).__init__()\n",
    "        self.in_channels = in_channels\n",
    "        self.num_classes = num_classes\n",
    "        self.FAM = CrossAttn(self.in_channels)\n",
    "        self.ARM = CrossAttn(self.in_channels)\n",
    "        self.edge_proj = nn.Linear(in_channels, in_channels)\n",
    "        self.bn = nn.BatchNorm2d(self.num_classes * self.num_classes)\n",
    "\n",
    "        self.edge_proj.weight.data.normal_(0, math.sqrt(2. / in_channels))\n",
    "        self.bn.weight.data.fill_(1)\n",
    "        self.bn.bias.data.zero_()\n",
    "\n",
    "    def forward(self, class_feature, global_feature):\n",
    "        B, N, D, C = class_feature.shape\n",
    "        global_feature = global_feature.repeat(1, N, 1).view(B, N, D, C)\n",
    "        feat = self.FAM(class_feature, global_feature)\n",
    "        feat_end = feat.repeat(1, 1, N, 1).view(B, -1, D, C)\n",
    "        feat_start = feat.repeat(1, N, 1, 1).view(B, -1, D, C)\n",
    "        feat = self.ARM(feat_start, feat_end)\n",
    "        edge = self.bn(self.edge_proj(feat))\n",
    "        return edge\n",
    "\n",
    "class LinearBlock(nn.Module):\n",
    "    def __init__(self, in_features,out_features=None,drop=0.0):\n",
    "        super().__init__()\n",
    "        out_features = out_features or in_features\n",
    "        self.fc = nn.Linear(in_features, out_features)\n",
    "        self.bn = nn.BatchNorm1d(out_features)\n",
    "        self.relu = nn.ReLU(inplace=True)\n",
    "        self.drop = nn.Dropout(drop)\n",
    "        self.fc.weight.data.normal_(0, math.sqrt(2. / out_features))\n",
    "        self.bn.weight.data.fill_(1)\n",
    "        self.bn.bias.data.zero_()\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.drop(x)\n",
    "        x = self.fc(x).permute(0, 2, 1)\n",
    "        x = self.relu(self.bn(x)).permute(0, 2, 1)\n",
    "        return x\n",
    "#Used in stage 1 (ANFL)\n",
    "def normalize_digraph(A):\n",
    "    b, n, _ = A.shape\n",
    "    node_degrees = A.detach().sum(dim = -1)\n",
    "    degs_inv_sqrt = node_degrees ** -0.5\n",
    "    norm_degs_matrix = torch.eye(n)\n",
    "    dev = A.get_device()\n",
    "    if dev >= 0:\n",
    "        norm_degs_matrix = norm_degs_matrix.to(dev)\n",
    "    norm_degs_matrix = norm_degs_matrix.view(1, n, n) * degs_inv_sqrt.view(b, n, 1)\n",
    "    norm_A = torch.bmm(torch.bmm(norm_degs_matrix,A),norm_degs_matrix)\n",
    "    return norm_A\n",
    "\n",
    "\n",
    "#Used in stage 2 (MEFL)\n",
    "def create_e_matrix(n):\n",
    "    end = torch.zeros((n*n,n))\n",
    "    for i in range(n):\n",
    "        end[i * n:(i + 1) * n, i] = 1\n",
    "    start = torch.zeros(n, n)\n",
    "    for i in range(n):\n",
    "        start[i, i] = 1\n",
    "    start = start.repeat(n,1)\n",
    "    return start,end\n",
    "class MEFG(nn.Module):\n",
    "    def __init__(self, in_channels, num_classes):\n",
    "        super(MEFG, self).__init__()\n",
    "        self.in_channels = in_channels\n",
    "        self.num_classes = num_classes\n",
    "        self.VCR = CrossAttn(self.in_channels)\n",
    "        self.VVR = CrossAttn(self.in_channels)\n",
    "        self.edge_proj = nn.Linear(in_channels, in_channels)\n",
    "        self.bn = nn.BatchNorm2d(self.num_classes * self.num_classes)\n",
    "\n",
    "        self.edge_proj.weight.data.normal_(0, math.sqrt(2. / in_channels))\n",
    "        self.bn.weight.data.fill_(1)\n",
    "        self.bn.bias.data.zero_()\n",
    "\n",
    "    def forward(self, class_feature, global_feature):\n",
    "        B, N, D, C = class_feature.shape\n",
    "        global_feature = global_feature.repeat(1, N, 1).view(B, N, D, C)\n",
    "        feat = self.VCR(class_feature, global_feature)\n",
    "        feat_end = feat.repeat(1, 1, N, 1).view(B, -1, D, C)\n",
    "        feat_start = feat.repeat(1, N, 1, 1).view(B, -1, D, C)\n",
    "        feat = self.VVR(feat_start, feat_end)\n",
    "        edge = self.bn(self.edge_proj(feat))\n",
    "        return edge\n",
    "class GRUModel(nn.Module):\n",
    "    def __init__(self, input_num, hidden_num, output_num):\n",
    "        super(GRUModel, self).__init__()\n",
    "        self.hidden_size = hidden_num\n",
    "        # 这里设置了 batch_first=True, 所以应该 inputs = inputs.view(inputs.shape[0], -1, inputs.shape[1])\n",
    "        # 针对时间序列预测问题，相当于将时间步（seq_len）设置为 1。\n",
    "        self.GRU_layer = nn.GRU(input_size=input_num, hidden_size=hidden_num, batch_first=True,bidirectional=True)\n",
    "        self.output_linear = nn.Linear(hidden_num*2, output_num)\n",
    "        self.hidden = None\n",
    "\n",
    "    def forward(self, x):\n",
    "        # h_n of shape (num_layers * num_directions, batch, hidden_size)\n",
    "        # 这里不用显式地传入隐层状态 self.hidden\n",
    "        x, self.hidden = self.GRU_layer(x)\n",
    "        # print(x.shape)\n",
    "        x = self.output_linear(x)\n",
    "        return x, self.hidden\n",
    "\n",
    "class GNN(nn.Module):\n",
    "    def __init__(self, in_channels, num_classes):\n",
    "        super(GNN, self).__init__()\n",
    "        self.in_channels = in_channels\n",
    "        self.num_classes = num_classes\n",
    "        # GNN Matrix: E x N\n",
    "        # Start Matrix Item:  define the source node of one edge\n",
    "        # End Matrix Item:  define the target node of one edge\n",
    "        # Algorithm details in Residual Gated Graph Convnets: arXiv preprint arXiv:1711.07553\n",
    "        # or Benchmarking Graph Neural Networks: arXiv preprint arXiv:2003.00982v3\n",
    "\n",
    "        start, end = create_e_matrix(self.num_classes)\n",
    "        self.start = Variable(start, requires_grad=False)\n",
    "        self.end = Variable(end, requires_grad=False)\n",
    "\n",
    "        dim_in = self.in_channels\n",
    "        dim_out = self.in_channels\n",
    "\n",
    "        self.U1 = nn.Linear(dim_in, dim_out, bias=False)\n",
    "        self.V1 = nn.Linear(dim_in, dim_out, bias=False)\n",
    "        self.A1 = nn.Linear(dim_in, dim_out, bias=False)\n",
    "        self.B1 = nn.Linear(dim_in, dim_out, bias=False)\n",
    "        self.E1 = nn.Linear(dim_in, dim_out, bias=False)\n",
    "\n",
    "        self.U2 = nn.Linear(dim_in, dim_out, bias=False)\n",
    "        self.V2 = nn.Linear(dim_in, dim_out, bias=False)\n",
    "        self.A2 = nn.Linear(dim_in, dim_out, bias=False)\n",
    "        self.B2 = nn.Linear(dim_in, dim_out, bias=False)\n",
    "        self.E2 = nn.Linear(dim_in, dim_out, bias=False)\n",
    "\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "        self.softmax = nn.Softmax(2)\n",
    "        self.bnv1 = nn.BatchNorm1d(num_classes)\n",
    "        self.bne1 = nn.BatchNorm1d(num_classes*num_classes)\n",
    "\n",
    "        self.bnv2 = nn.BatchNorm1d(num_classes)\n",
    "        self.bne2 = nn.BatchNorm1d(num_classes * num_classes)\n",
    "\n",
    "        self.act = nn.ReLU()\n",
    "\n",
    "        self.init_weights_linear(dim_in, 1)\n",
    "\n",
    "    def init_weights_linear(self, dim_in, gain):\n",
    "        # conv1\n",
    "        scale = gain * np.sqrt(2.0 / dim_in)\n",
    "        self.U1.weight.data.normal_(0, scale)\n",
    "        self.V1.weight.data.normal_(0, scale)\n",
    "        self.A1.weight.data.normal_(0, scale)\n",
    "        self.B1.weight.data.normal_(0, scale)\n",
    "        self.E1.weight.data.normal_(0, scale)\n",
    "\n",
    "        self.U2.weight.data.normal_(0, scale)\n",
    "        self.V2.weight.data.normal_(0, scale)\n",
    "        self.A2.weight.data.normal_(0, scale)\n",
    "        self.B2.weight.data.normal_(0, scale)\n",
    "        self.E2.weight.data.normal_(0, scale)\n",
    "\n",
    "        bn_init(self.bnv1)\n",
    "        bn_init(self.bne1)\n",
    "        bn_init(self.bnv2)\n",
    "        bn_init(self.bne2)\n",
    "\n",
    "    def forward(self, x, edge):\n",
    "        # device\n",
    "        dev = x.get_device()\n",
    "        if dev >= 0:\n",
    "            start = self.start.to(dev)\n",
    "            end = self.end.to(dev)\n",
    "\n",
    "        # GNN Layer 1:\n",
    "        res = x\n",
    "        Vix = self.A1(x)  # V x d_out\n",
    "        Vjx = self.B1(x)  # V x d_out\n",
    "        e = self.E1(edge)  # E x d_out\n",
    "        edge = edge + self.act(self.bne1(torch.einsum('ev, bvc -> bec', (end, Vix)) + torch.einsum('ev, bvc -> bec',(start, Vjx)) + e))  # E x d_out\n",
    "\n",
    "        e = self.sigmoid(edge)\n",
    "        b, _, c = e.shape\n",
    "        e = e.view(b,self.num_classes, self.num_classes, c)\n",
    "        e = self.softmax(e)\n",
    "        e = e.view(b, -1, c)\n",
    "\n",
    "        Ujx = self.V1(x)  # V x H_out\n",
    "        Ujx = torch.einsum('ev, bvc -> bec', (start, Ujx))  # E x H_out\n",
    "        Uix = self.U1(x)  # V x H_out\n",
    "        x = Uix + torch.einsum('ve, bec -> bvc', (end.t(), e * Ujx)) / self.num_classes  # V x H_out\n",
    "        x = self.act(res + self.bnv1(x))\n",
    "        res = x\n",
    "\n",
    "        # GNN Layer 2:\n",
    "        Vix = self.A2(x)  # V x d_out\n",
    "        Vjx = self.B2(x)  # V x d_out\n",
    "        e = self.E2(edge)  # E x d_out\n",
    "        edge = edge + self.act(self.bne2(torch.einsum('ev, bvc -> bec', (end, Vix)) + torch.einsum('ev, bvc -> bec', (start, Vjx)) + e))  # E x d_out\n",
    "\n",
    "        e = self.sigmoid(edge)\n",
    "        b, _, c = e.shape\n",
    "        e = e.view(b, self.num_classes, self.num_classes, c)\n",
    "        e = self.softmax(e)\n",
    "        e = e.view(b, -1, c)\n",
    "\n",
    "        Ujx = self.V2(x)  # V x H_out\n",
    "        Ujx = torch.einsum('ev, bvc -> bec', (start, Ujx))  # E x H_out\n",
    "        Uix = self.U2(x)  # V x H_out\n",
    "        x = Uix + torch.einsum('ve, bec -> bvc', (end.t(), e * Ujx)) / self.num_classes  # V x H_out\n",
    "        x = self.act(res + self.bnv2(x))\n",
    "        return x, edge\n",
    "\n",
    "class GNNMASK(nn.Module):\n",
    "    def __init__(self, in_channels, num_classes=10, neighbor_num=1, metric='dots'):\n",
    "        super(GNNMASK, self).__init__()\n",
    "        # in_channels: dim of node feature\n",
    "        # num_classes: num of nodes\n",
    "        # neighbor_num: K in paper and we select the top-K nearest neighbors for each node feature.\n",
    "        # metric: metric for assessing node similarity. Used in FGG module to build a dynamical graph\n",
    "        # X' = ReLU(X + BN(V(X) + A x U(X)) )\n",
    "\n",
    "        self.in_channels = in_channels\n",
    "        self.num_classes = num_classes\n",
    "        self.relu = nn.ReLU()\n",
    "        self.metric = metric\n",
    "        self.neighbor_num = neighbor_num\n",
    "\n",
    "#         # network\n",
    "        self.U = nn.Linear(self.in_channels,self.in_channels)\n",
    "        self.V = nn.Linear(self.in_channels,self.in_channels)\n",
    "        self.bnv = nn.BatchNorm1d(num_classes)\n",
    "\n",
    "# #         # init\n",
    "        self.U.weight.data.normal_(0, math.sqrt(2. / self.in_channels))\n",
    "        self.V.weight.data.normal_(0, math.sqrt(2. / self.in_channels))\n",
    "        self.bnv.weight.data.fill_(1)\n",
    "        self.bnv.bias.data.zero_()\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "\n",
    "    def forward(self, x):\n",
    "        b, n, c = x.shape\n",
    "\n",
    "        # build dynamical graph\n",
    "        # si = x.detach()\n",
    "\n",
    "# build dynamical graph\n",
    "        if self.metric == 'dots':\n",
    "            si = x.detach()\n",
    "            si = torch.einsum('b i j , b j k -> b i k', si, si.transpose(1, 2))\n",
    "            threshold = si.topk(k=self.neighbor_num, dim=-1, largest=True)[0][:, :, -1].view(b, n, 1)\n",
    "            adj = (si >= threshold).float()\n",
    "\n",
    "        elif self.metric == 'cosine':\n",
    "            si = x.detach()\n",
    "            si = F.normalize(si, p=2, dim=-1)\n",
    "            si = torch.einsum('b i j , b j k -> b i k', si, si.transpose(1, 2))\n",
    "            threshold = si.topk(k=self.neighbor_num, dim=-1, largest=True)[0][:, :, -1].view(b, n, 1)\n",
    "            adj = (si >= threshold).float()\n",
    "\n",
    "        elif self.metric == 'l1':\n",
    "            si = x.detach().repeat(1, n, 1).view(b, n, n, c)\n",
    "            si = torch.abs(si.transpose(1, 2) - si)\n",
    "            si = si.sum(dim=-1)\n",
    "            threshold = si.topk(k=self.neighbor_num, dim=-1, largest=False)[0][:, :, -1].view(b, n, 1)\n",
    "            adj = (si <= threshold).float()\n",
    "\n",
    "        else:\n",
    "            raise Exception(\"Error: wrong metric: \", self.metric)\n",
    "\n",
    "\n",
    "        # si_1 = torch.einsum('b i j , b j k -> b i k', si, si.transpose(1, 2))\n",
    "        # adj_1 = self.sigmoid(si_1.float())\n",
    "\n",
    "        # si = x.detach()\n",
    "        # si = torch.einsum('b i j , b j k -> b i k', si, si.transpose(1, 2))\n",
    "        # threshold = si.topk(k=self.neighbor_num, dim=-1, largest=True)[0][:, :, -1].view(b, n, 1)\n",
    "        # adj = (si <= threshold).float()\n",
    "        # GNN process\n",
    "        A = normalize_digraph(adj)\n",
    "        aggregate = torch.einsum('b i j, b j k->b i k', adj, self.V(x))\n",
    "        x = self.relu(x + self.bnv(aggregate + self.U(x)))\n",
    "        # si = x.detach()\n",
    "        si = torch.einsum('b i j , b j k -> b i k', x, x.transpose(1, 2))\n",
    "        # adj=si\n",
    "        A = normalize_digraph(si)\n",
    "        return x\n",
    "class Head(nn.Module):\n",
    "    def __init__(self, in_channels, num_classes):\n",
    "        super(Head, self).__init__()\n",
    "        # The head of network\n",
    "        # Input: the feature maps x from backbone\n",
    "        # Output: the AU recognition probabilities cl And the logits cl_edge of edge features for classification\n",
    "        # Modules: 1. AFG extracts individual Au feature maps U_1 ---- U_N\n",
    "        #          2. MEFG: graph edge modeling for learning multi-dimensional edge features\n",
    "        #          3. Gated-GCN for graph learning with node and multi-dimensional edge features\n",
    "        # sc: individually calculate cosine similarity between node features and a trainable vector.\n",
    "        # edge fc: for edge prediction\n",
    "\n",
    "        self.in_channels = in_channels\n",
    "        self.num_classes = num_classes\n",
    "        class_linear_layers = []\n",
    "        for i in range(self.num_classes):\n",
    "            layer = LinearBlock(self.in_channels, self.in_channels)\n",
    "            class_linear_layers += [layer]\n",
    "        self.class_linears = nn.ModuleList(class_linear_layers)\n",
    "        self.edge_extractor = MEFG(self.in_channels, self.num_classes)\n",
    "        self.gnn = GNN(self.in_channels, self.num_classes)\n",
    "        self.sc = nn.Parameter(torch.FloatTensor(torch.zeros(self.num_classes, self.in_channels)))\n",
    "        self.edge_fc = nn.Linear(self.in_channels, 4)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.mask = GNNMASK(in_channels, num_classes)\n",
    "        nn.init.xavier_uniform_(self.edge_fc.weight)\n",
    "        nn.init.xavier_uniform_(self.sc)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # AFG\n",
    "        f_u = []\n",
    "        for i, layer in enumerate(self.class_linears):\n",
    "            f_u.append(layer(x).unsqueeze(1))\n",
    "        f_u = torch.cat(f_u, dim=1)\n",
    "        #print(f_u.shape)\n",
    "        f_v = f_u.mean(dim=-2)\n",
    "        # f_e = self.edge_extractor(f_u, x)\n",
    "        # f_e = f_e.mean(dim=-2)\n",
    " \n",
    "        \n",
    "        # Predefined Rules\n",
    "        # print(f_v.shape)#1750,10,40\n",
    "        # f_e = torch.zeros(f_v.shape[0], 100, 40).cpu().detach().numpy()\n",
    "        # # for m in range(f_v.shape[0]):\n",
    "        #     for i in range(4):\n",
    "        #         for j in range(4):\n",
    "        #             a = f_v[m,i,:]\n",
    "        #             b = f_v[m,j,:]\n",
    "        #             f_e[m,i*j,:].fill(distance.euclidean(a.cpu().detach().numpy(), b.cpu().detach().numpy()))\n",
    "\n",
    "        # TTP\n",
    "        f_v = self.mask(f_v)\n",
    "\n",
    "        # MEFL\n",
    "        # f_e = self.edge_extractor(f_u, x)\n",
    "        # feat_end = f_v.repeat(1, 1, n).view(b, -1, c)\n",
    "        # feat_start = f_v.repeat(1, n, 1).view(b, -1, c)\n",
    "\n",
    "        # MEFL\n",
    "        # f_e = self.edge_extractor(f_u, x)\n",
    "        # f_e = f_e.mean(dim=-2)\n",
    "\n",
    "        # feat_end = f_v.repeat(1, 1, n).view(b, -1, c)\n",
    "        # feat_start = f_v.repeat(1, n, 1).view(b, -1, c)\n",
    "        # f_e = feat_start - feat_end\n",
    "\n",
    "        b, n, c = f_v.shape\n",
    "        # mask = A.view(b,n*n,1)\n",
    "        # f_e=f_e*mask\n",
    "        # f_v, f_e = self.gnn(f_v, f_e)\n",
    "        b, n, c = f_v.shape\n",
    "        sc = self.sc\n",
    "        sc = self.relu(sc)\n",
    "        sc = F.normalize(sc, p=2, dim=-1)\n",
    "        cl = F.normalize(f_v, p=2, dim=-1)\n",
    "        cl = (cl * sc.view(1, n, c)).sum(dim=-1)\n",
    "\n",
    "        # sc = self.sc\n",
    "        # sc = self.relu(sc)\n",
    "        # sc = F.normalize(sc, p=2, dim=-1)\n",
    "        # cl = F.normalize(f_v, p=2, dim=-1)\n",
    "        # cl = (cl * sc.view(1, n, c)).sum(dim=-1, keepdim=False)\n",
    "        # cl_edge = self.edge_fc(f_e)\n",
    "        return cl \n",
    "class MEFARG(nn.Module):\n",
    "    def __init__(self, num_classes=10, neighbor_num=1,metric='dots'):\n",
    "        super(MEFARG, self).__init__()\n",
    "        self.bigru = GRUModel(num_classes,num_classes,num_classes).to(device)\n",
    "        self.bigru2 = GRUModel(num_classes,num_classes,num_classes).to(device)\n",
    "        self.bigru1 = GRUModel(32,32,32).to(device)\n",
    "        self.in_channels = 32\n",
    "        self.out_channels = self.in_channels // 4\n",
    "        self.global_linear = LinearBlock(self.in_channels, self.out_channels)\n",
    "        self.head = Head(self.out_channels, num_classes)\n",
    "        self.linear = nn.Linear(num_classes, 1).to(device)\n",
    "        self.linear2 = nn.Linear(160, 1).to(device)\n",
    "    def forward(self, x):\n",
    "        # x: b d c\n",
    "        # x,_ = self.bigru(torch.unsqueeze(x,dim=0))\n",
    "        x,_ = self.bigru1(torch.transpose(x,0,1))\n",
    "        # print(x.shape)\n",
    "        x = torch.transpose(x,0,1)\n",
    "        # print(x.shape)\n",
    "        \n",
    "        # cl_5 = self.linear2(torch.transpose(x,0,1)).reshape((5,-1))\n",
    "        # x = x.reshape(-1,1,160)\n",
    "        # x = torch.unsqueeze(x,dim=0)\n",
    "        x = self.global_linear(x)\n",
    "        # print(x.shape)\n",
    "        \n",
    "        # print(x.shape)\n",
    "        # cl = torch.unsqueeze(self.linear(x),dim=-1)\n",
    "        cl = self.head(x)\n",
    "        cl,_ = self.bigru(torch.unsqueeze(cl,dim=0))\n",
    "        feat = cl\n",
    "        # cl,_ = self.bigru2(cl)\n",
    "        # print(cl.shape)\n",
    "        cl = torch.unsqueeze(self.linear(cl).flatten(),dim=0)\n",
    "        # print(cl.shape)\n",
    "        # print(x1)\n",
    "        return cl,feat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d5c6d3a4-f027-428a-b4d0-ffc81d09b113",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "def train_AMEF_with_result(Culture,label,learning_rate,Test_CH=False,Test_BR=False):\n",
    "    EPOCHS = 100\n",
    "    # learning_rate = 0.01\n",
    "    max_length = 1768\n",
    "    train_file_list = []\n",
    "    dev_file_list = []\n",
    "    train_y_list = []\n",
    "    dev_y_list = []\n",
    "    for filename in os.listdir('./AVEC2019_CES_traindevel/audio/'):\n",
    "        if Culture =='DE+HU':\n",
    "            if 'Train' in filename:\n",
    "                audio_features_deepspectrum = pd.read_csv('./AVEC2019-master/Baseline_systems/CES/output/deepspectrum/'+filename.split('.')[0]+'.csv').iloc[:,:].values\n",
    "                audio_features_egemaps_xbow = pd.read_csv('./AVEC2019-master/Baseline_systems/CES/output/egemaps_xbow/'+filename.split('.')[0]+'.csv').iloc[:,:].values\n",
    "                audio_features_egemaps_functionals = pd.read_csv('./AVEC2019-master/Baseline_systems/CES/output/egemaps_functionals/'+filename.split('.')[0]+'.csv').iloc[:,:].values\n",
    "                audio_features_mfcc_xbow = pd.read_csv('./AVEC2019-master/Baseline_systems/CES/output/mfcc_xbow/'+filename.split('.')[0]+'.csv').iloc[:,:].values\n",
    "                audio_features_mfcc_functionals = pd.read_csv('./AVEC2019-master/Baseline_systems/CES/output/mfcc_functionals/'+filename.split('.')[0]+'.csv').iloc[:,:].values\n",
    "                # audio_features_deepspectrum = (audio_features_deepspectrum-audio_features_deepspectrum.mean())/(audio_features_deepspectrum.max()-audio_features_deepspectrum.min())\n",
    "                # audio_features_egemaps_xbow = (audio_features_egemaps_xbow-audio_features_egemaps_xbow.mean())/(audio_features_egemaps_xbow.max()-audio_features_egemaps_xbow.min())\n",
    "                # audio_features_egemaps_functionals = (audio_features_egemaps_functionals - audio_features_egemaps_functionals.mean())/(audio_features_egemaps_functionals.max()-audio_features_egemaps_functionals.min())\n",
    "                # audio_features_mfcc_xbow = (audio_features_mfcc_xbow - audio_features_mfcc_xbow.mean())/(audio_features_mfcc_xbow.max()-audio_features_mfcc_xbow.min())\n",
    "                # audio_features_mfcc_functionals = (audio_features_mfcc_functionals- audio_features_mfcc_functionals.mean())/(audio_features_mfcc_functionals.max()-audio_features_mfcc_functionals.min())\n",
    "\n",
    "                min_shape = min(audio_features_deepspectrum.shape[0],audio_features_egemaps_xbow.shape[0],audio_features_egemaps_functionals.shape[0],audio_features_mfcc_xbow.shape[0],audio_features_mfcc_functionals.shape[0])\n",
    "                all_feat = np.concatenate([audio_features_deepspectrum[:min_shape,],audio_features_egemaps_xbow[:min_shape,],audio_features_egemaps_functionals[:min_shape,],audio_features_mfcc_xbow[:min_shape,],audio_features_mfcc_functionals[:min_shape,]],axis=1)\n",
    "                # all_feat = audio_features_deepspectrum[:min_shape,]+audio_features_egemaps_xbow[:min_shape,]+audio_features_egemaps_functionals[:min_shape,]+audio_features_mfcc_xbow[:min_shape,]+audio_features_mfcc_functionals[:min_shape,]\n",
    "                # all_feat = (all_feat)/(all_feat.max()-all_feat.min())\n",
    "                if all_feat.shape[0]>=max_length:\n",
    "                    all_feat = all_feat[:max_length,:]\n",
    "                else:\n",
    "                    all_feat = np.concatenate([all_feat,np.zeros((max_length-all_feat.shape[0],all_feat.shape[1]))],axis=0)\n",
    "                all_feat = torch.tensor(all_feat,dtype=torch.float32)\n",
    "                train_file_list.append(all_feat)\n",
    "                y = pd.read_csv('./AVEC2019_CES_traindevel/labels/'+filename.split('.')[0]+'.csv',sep=';')\n",
    "                y = torch.tensor(y[label].values,dtype=torch.float32)\n",
    "                train_y_list.append(y)\n",
    "            else:\n",
    "                audio_features_deepspectrum = pd.read_csv('./AVEC2019-master/Baseline_systems/CES/output/deepspectrum/'+filename.split('.')[0]+'.csv').iloc[:,:].values\n",
    "                audio_features_egemaps_xbow = pd.read_csv('./AVEC2019-master/Baseline_systems/CES/output/egemaps_xbow/'+filename.split('.')[0]+'.csv').iloc[:,:].values\n",
    "                audio_features_egemaps_functionals = pd.read_csv('./AVEC2019-master/Baseline_systems/CES/output/egemaps_functionals/'+filename.split('.')[0]+'.csv').iloc[:,:].values\n",
    "                audio_features_mfcc_xbow = pd.read_csv('./AVEC2019-master/Baseline_systems/CES/output/mfcc_xbow/'+filename.split('.')[0]+'.csv').iloc[:,:].values\n",
    "                audio_features_mfcc_functionals = pd.read_csv('./AVEC2019-master/Baseline_systems/CES/output/mfcc_functionals/'+filename.split('.')[0]+'.csv').iloc[:,:].values\n",
    "                # audio_features_deepspectrum = (audio_features_deepspectrum-audio_features_deepspectrum.mean())/(audio_features_deepspectrum.max()-audio_features_deepspectrum.min())\n",
    "                # audio_features_egemaps_xbow = (audio_features_egemaps_xbow-audio_features_egemaps_xbow.mean())/(audio_features_egemaps_xbow.max()-audio_features_egemaps_xbow.min())\n",
    "                # audio_features_egemaps_functionals = (audio_features_egemaps_functionals - audio_features_egemaps_functionals.mean())/(audio_features_egemaps_functionals.max()-audio_features_egemaps_functionals.min())\n",
    "                # audio_features_mfcc_xbow = (audio_features_mfcc_xbow - audio_features_mfcc_xbow.mean())/(audio_features_mfcc_xbow.max()-audio_features_mfcc_xbow.min())\n",
    "                # audio_features_mfcc_functionals = (audio_features_mfcc_functionals- audio_features_mfcc_functionals.mean())/(audio_features_mfcc_functionals.max()-audio_features_mfcc_functionals.min())\n",
    "\n",
    "                min_shape = min(audio_features_deepspectrum.shape[0],audio_features_egemaps_xbow.shape[0],audio_features_egemaps_functionals.shape[0],audio_features_mfcc_xbow.shape[0],audio_features_mfcc_functionals.shape[0])\n",
    "                all_feat = np.concatenate([audio_features_deepspectrum[:min_shape,],audio_features_egemaps_xbow[:min_shape,],audio_features_egemaps_functionals[:min_shape,],audio_features_mfcc_xbow[:min_shape,],audio_features_mfcc_functionals[:min_shape,]],axis=1)\n",
    "                # all_feat = audio_features_deepspectrum[:min_shape,]+audio_features_egemaps_xbow[:min_shape,]+audio_features_egemaps_functionals[:min_shape,]+audio_features_mfcc_xbow[:min_shape,]+audio_features_mfcc_functionals[:min_shape,]\n",
    "                # all_feat = (all_feat)/(all_feat.max()-all_feat.min())\n",
    "                if all_feat.shape[0]>=max_length:\n",
    "                    all_feat = all_feat[:max_length,:]\n",
    "                else:\n",
    "                    all_feat = np.concatenate([all_feat,np.zeros((max_length-all_feat.shape[0],all_feat.shape[1]))],axis=0)\n",
    "                all_feat = torch.tensor(all_feat,dtype=torch.float32)\n",
    "                dev_file_list.append(all_feat)\n",
    "                y = pd.read_csv('./AVEC2019_CES_traindevel/labels/'+filename.split('.')[0]+'.csv',sep=';')\n",
    "                y = torch.tensor(y[label].values,dtype=torch.float32)\n",
    "                dev_y_list.append(y)\n",
    "        else:\n",
    "            if Culture in filename:\n",
    "                if 'Train' in filename:\n",
    "                    audio_features_deepspectrum = pd.read_csv('./AVEC2019-master/Baseline_systems/CES/output/deepspectrum/'+filename.split('.')[0]+'.csv').iloc[:,:].values\n",
    "                    audio_features_egemaps_xbow = pd.read_csv('./AVEC2019-master/Baseline_systems/CES/output/egemaps_xbow/'+filename.split('.')[0]+'.csv').iloc[:,:].values\n",
    "                    audio_features_egemaps_functionals = pd.read_csv('./AVEC2019-master/Baseline_systems/CES/output/egemaps_functionals/'+filename.split('.')[0]+'.csv').iloc[:,:].values\n",
    "                    audio_features_mfcc_xbow = pd.read_csv('./AVEC2019-master/Baseline_systems/CES/output/mfcc_xbow/'+filename.split('.')[0]+'.csv').iloc[:,:].values\n",
    "                    audio_features_mfcc_functionals = pd.read_csv('./AVEC2019-master/Baseline_systems/CES/output/mfcc_functionals/'+filename.split('.')[0]+'.csv').iloc[:,:].values\n",
    "                    # audio_features_deepspectrum = (audio_features_deepspectrum-audio_features_deepspectrum.mean())/(audio_features_deepspectrum.max()-audio_features_deepspectrum.min())\n",
    "                    # audio_features_egemaps_xbow = (audio_features_egemaps_xbow-audio_features_egemaps_xbow.mean())/(audio_features_egemaps_xbow.max()-audio_features_egemaps_xbow.min())\n",
    "                    # audio_features_egemaps_functionals = (audio_features_egemaps_functionals - audio_features_egemaps_functionals.mean())/(audio_features_egemaps_functionals.max()-audio_features_egemaps_functionals.min())\n",
    "                    # audio_features_mfcc_xbow = (audio_features_mfcc_xbow - audio_features_mfcc_xbow.mean())/(audio_features_mfcc_xbow.max()-audio_features_mfcc_xbow.min())\n",
    "                    # audio_features_mfcc_functionals = (audio_features_mfcc_functionals- audio_features_mfcc_functionals.mean())/(audio_features_mfcc_functionals.max()-audio_features_mfcc_functionals.min())\n",
    "\n",
    "                    min_shape = min(audio_features_deepspectrum.shape[0],audio_features_egemaps_xbow.shape[0],audio_features_egemaps_functionals.shape[0],audio_features_mfcc_xbow.shape[0],audio_features_mfcc_functionals.shape[0])\n",
    "                    all_feat = np.concatenate([audio_features_deepspectrum[:min_shape,],audio_features_egemaps_xbow[:min_shape,],audio_features_egemaps_functionals[:min_shape,],audio_features_mfcc_xbow[:min_shape,],audio_features_mfcc_functionals[:min_shape,]],axis=1)\n",
    "                    # all_feat = audio_features_deepspectrum[:min_shape,]+audio_features_egemaps_xbow[:min_shape,]+audio_features_egemaps_functionals[:min_shape,]+audio_features_mfcc_xbow[:min_shape,]+audio_features_mfcc_functionals[:min_shape,]\n",
    "                    # all_feat = (all_feat)/(all_feat.max()-all_feat.min())\n",
    "                    if all_feat.shape[0]>=max_length:\n",
    "                        all_feat = all_feat[:max_length,:]\n",
    "                    else:\n",
    "                        all_feat = np.concatenate([all_feat,np.zeros((max_length-all_feat.shape[0],all_feat.shape[1]))],axis=0)\n",
    "                    all_feat = torch.tensor(all_feat,dtype=torch.float32)\n",
    "                    train_file_list.append(all_feat)\n",
    "                    y = pd.read_csv('./AVEC2019_CES_traindevel/labels/'+filename.split('.')[0]+'.csv',sep=';')\n",
    "                    y = torch.tensor(y[label].values,dtype=torch.float32)\n",
    "                    train_y_list.append(y)\n",
    "                else:\n",
    "                    audio_features_deepspectrum = pd.read_csv('./AVEC2019-master/Baseline_systems/CES/output/deepspectrum/'+filename.split('.')[0]+'.csv').iloc[:,:].values\n",
    "                    audio_features_egemaps_xbow = pd.read_csv('./AVEC2019-master/Baseline_systems/CES/output/egemaps_xbow/'+filename.split('.')[0]+'.csv').iloc[:,:].values\n",
    "                    audio_features_egemaps_functionals = pd.read_csv('./AVEC2019-master/Baseline_systems/CES/output/egemaps_functionals/'+filename.split('.')[0]+'.csv').iloc[:,:].values\n",
    "                    audio_features_mfcc_xbow = pd.read_csv('./AVEC2019-master/Baseline_systems/CES/output/mfcc_xbow/'+filename.split('.')[0]+'.csv').iloc[:,:].values\n",
    "                    audio_features_mfcc_functionals = pd.read_csv('./AVEC2019-master/Baseline_systems/CES/output/mfcc_functionals/'+filename.split('.')[0]+'.csv').iloc[:,:].values\n",
    "                    # audio_features_deepspectrum = (audio_features_deepspectrum-audio_features_deepspectrum.mean())/(audio_features_deepspectrum.max()-audio_features_deepspectrum.min())\n",
    "                    # audio_features_egemaps_xbow = (audio_features_egemaps_xbow-audio_features_egemaps_xbow.mean())/(audio_features_egemaps_xbow.max()-audio_features_egemaps_xbow.min())\n",
    "                    # audio_features_egemaps_functionals = (audio_features_egemaps_functionals - audio_features_egemaps_functionals.mean())/(audio_features_egemaps_functionals.max()-audio_features_egemaps_functionals.min())\n",
    "                    # audio_features_mfcc_xbow = (audio_features_mfcc_xbow - audio_features_mfcc_xbow.mean())/(audio_features_mfcc_xbow.max()-audio_features_mfcc_xbow.min())\n",
    "                    # audio_features_mfcc_functionals = (audio_features_mfcc_functionals- audio_features_mfcc_functionals.mean())/(audio_features_mfcc_functionals.max()-audio_features_mfcc_functionals.min())\n",
    "\n",
    "                    min_shape = min(audio_features_deepspectrum.shape[0],audio_features_egemaps_xbow.shape[0],audio_features_egemaps_functionals.shape[0],audio_features_mfcc_xbow.shape[0],audio_features_mfcc_functionals.shape[0])\n",
    "                    all_feat = np.concatenate([audio_features_deepspectrum[:min_shape,],audio_features_egemaps_xbow[:min_shape,],audio_features_egemaps_functionals[:min_shape,],audio_features_mfcc_xbow[:min_shape,],audio_features_mfcc_functionals[:min_shape,]],axis=1)\n",
    "                    # all_feat = audio_features_deepspectrum[:min_shape,]+audio_features_egemaps_xbow[:min_shape,]+audio_features_egemaps_functionals[:min_shape,]+audio_features_mfcc_xbow[:min_shape,]+audio_features_mfcc_functionals[:min_shape,]\n",
    "                    # all_feat = (all_feat)/(all_feat.max()-all_feat.min())\n",
    "                    if all_feat.shape[0]>=max_length:\n",
    "                        all_feat = all_feat[:max_length,:]\n",
    "                    else:\n",
    "                        all_feat = np.concatenate([all_feat,np.zeros((max_length-all_feat.shape[0],all_feat.shape[1]))],axis=0)\n",
    "                    all_feat = torch.tensor(all_feat,dtype=torch.float32)\n",
    "                    dev_file_list.append(all_feat)\n",
    "                    y = pd.read_csv('./AVEC2019_CES_traindevel/labels/'+filename.split('.')[0]+'.csv',sep=';')\n",
    "                    y = torch.tensor(y[label].values,dtype=torch.float32)\n",
    "                    dev_y_list.append(y)\n",
    "    emotion_model = MEFARG().to(device)\n",
    "    loss_fn = ConcordanceCC()\n",
    "    optimizer = torch.optim.RMSprop(emotion_model.parameters(),lr = learning_rate)\n",
    "    max_ccc = 0\n",
    "    max_ch_ccc = 0\n",
    "    max_br_ccc = 0\n",
    "    train_time = []\n",
    "    infer_time = []\n",
    "    train_loss = []\n",
    "    infer_loss = []\n",
    "    for epoch in range(EPOCHS):\n",
    "        emotion_model.train()\n",
    "        begin_one_epoch_train = time.time()\n",
    "        for all_feat,y in zip(train_file_list,train_y_list):\n",
    "            optimizer.zero_grad()\n",
    "            all_feat = torch.transpose(torch.reshape(all_feat,[all_feat.shape[0],32,5]),1,2)\n",
    "            y_pred,y_pred_5 = emotion_model(all_feat.to(device))\n",
    "            # y_pred = (y_pred-y_pred.mean())/(y_pred.max()-y_pred.min())\n",
    "\n",
    "            y_true = torch.unsqueeze(y,axis=0).to(device)\n",
    "            if y_true.shape[1] < max_length:\n",
    "                y_true = torch.cat([y_true,torch.zeros((1,max_length-y_true.shape[1])).to(device)],axis=1)\n",
    "            else:\n",
    "                y_true = y_true[:,:max_length]\n",
    "            cccloss = 2-loss_fn(torch.unsqueeze(y_pred.flatten(),axis=0),y_true)#-loss_fn(y_pred_5,torch.cat([y_true for i in range(5)],dim=0))\n",
    "            cccloss.backward()\n",
    "            optimizer.step()#参数更新\n",
    "        end_one_epoch_train = time.time()\n",
    "        train_time.append(end_one_epoch_train-begin_one_epoch_train)\n",
    "        begin_one_epoch_infer = time.time()\n",
    "        emotion_model.eval()\n",
    "        with torch.no_grad():\n",
    "            dev_cc_loss = 0.0\n",
    "            y_l = []\n",
    "            feat_y_l = []\n",
    "            for all_feat,y in zip(dev_file_list,dev_y_list):\n",
    "                y_l += y.data.cpu().numpy().tolist()\n",
    "                shape_y = y.shape[0]\n",
    "                all_feat = torch.transpose(torch.reshape(all_feat,[all_feat.shape[0],32,5]),1,2)\n",
    "                y_pred,y_pred_5 = emotion_model(all_feat.to(device))\n",
    "                # y_pred = (y_pred-y_pred.mean())/(y_pred.max()-y_pred.min())\n",
    "                # y_pred = torch.unsqueeze(y_pred_5.mean(0),dim=0)\n",
    "                y_true = torch.unsqueeze(y,axis=0).to(device)\n",
    "                if y_true.shape[1] < max_length:\n",
    "                    y_true = torch.cat([y_true,torch.zeros((1,max_length-y_true.shape[1])).to(device)],axis=1)\n",
    "                else:\n",
    "                    y_true = y_true[:,:max_length]\n",
    "                cccloss = loss_fn(torch.unsqueeze(y_pred.flatten(),axis=0),y_true)\n",
    "                dev_cc_loss += cccloss.item()\n",
    "                feat_y_l.append(y_pred_5[0,:shape_y,:].data.cpu().numpy())\n",
    "            feat_y = np.concatenate(feat_y_l,axis=0)\n",
    "            feat_df = pd.DataFrame()\n",
    "            feat_df[label] = y_l\n",
    "            for i in range(feat_y.shape[1]):\n",
    "                feat_df['feat_'+str(i)] = feat_y[:,i]\n",
    "            ccc_loss = dev_cc_loss/len(dev_file_list)\n",
    "            print(f'dev_CCC_loss:{ccc_loss}')\n",
    "            if ccc_loss>max_ccc:\n",
    "                max_ccc = ccc_loss\n",
    "                torch.save(emotion_model.state_dict(), f'./saved_model_TTP/{Culture}_{label}_{ccc_loss}_model.bin')\n",
    "                feat_df.to_csv(f'./saved_feat_TTP/{Culture}_{label}_{ccc_loss}.csv',index=0)\n",
    "        end_one_epoch_infer = time.time()\n",
    "        infer_time.append(end_one_epoch_infer-begin_one_epoch_infer)\n",
    "        infer_loss.append(ccc_loss)\n",
    "        with torch.no_grad():\n",
    "            train_cc_loss = 0.0\n",
    "            y_l = []\n",
    "            for all_feat,y in zip(train_file_list,train_y_list):\n",
    "                y_l += y.data.cpu().numpy().tolist()\n",
    "                shape_y = y.shape[0]\n",
    "                all_feat = torch.transpose(torch.reshape(all_feat,[all_feat.shape[0],32,5]),1,2)\n",
    "                y_pred,y_pred_5 = emotion_model(all_feat.to(device))\n",
    "                y_true = torch.unsqueeze(y,axis=0).to(device)\n",
    "                if y_true.shape[1] < max_length:\n",
    "                    y_true = torch.cat([y_true,torch.zeros((1,max_length-y_true.shape[1])).to(device)],axis=1)\n",
    "                else:\n",
    "                    y_true = y_true[:,:max_length]\n",
    "                cccloss = loss_fn(torch.unsqueeze(y_pred.flatten(),axis=0),y_true)\n",
    "                train_cc_loss += cccloss.item()\n",
    "            ccc_loss = train_cc_loss/len(train_file_list)\n",
    "        train_loss.append(ccc_loss)\n",
    "    result = pd.DataFrame()\n",
    "    result['train_time'] = train_time\n",
    "    result['infer_time'] = infer_time\n",
    "    result['train_loss'] = train_loss\n",
    "    result['infer_loss'] = infer_loss\n",
    "    result.to_csv(f'./saved_result_TTP/{Culture}_{label}_{max_ccc}.csv',index=0)\n",
    "    print(f'{Culture}__{label}__ccc_loss:{max_ccc}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6d95daab-ed9b-4ec4-8014-52e8669647ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Cross_validation\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "import time\n",
    "def train_AMEF_Cross_validation(Culture,label,learning_rate,fold=12,Test_CH=False,Test_BR=False):\n",
    "    EPOCHS = 50\n",
    "    # learning_rate = 0.01\n",
    "    max_length = 1768\n",
    "    train_file_list = []\n",
    "    dev_file_list = []\n",
    "    train_y_list = []\n",
    "    dev_y_list = []\n",
    "    data_process_begin = time.time()\n",
    "    \n",
    "    for filename in os.listdir('./AVEC2019_CES_traindevel/audio/'):\n",
    "        if Culture =='DE+HU':\n",
    "            if 'Train' in filename:\n",
    "                audio_features_deepspectrum = pd.read_csv('./AVEC2019-master/Baseline_systems/CES/output/deepspectrum/'+filename.split('.')[0]+'.csv').iloc[:,:].values\n",
    "                audio_features_egemaps_xbow = pd.read_csv('./AVEC2019-master/Baseline_systems/CES/output/egemaps_xbow/'+filename.split('.')[0]+'.csv').iloc[:,:].values\n",
    "                audio_features_egemaps_functionals = pd.read_csv('./AVEC2019-master/Baseline_systems/CES/output/egemaps_functionals/'+filename.split('.')[0]+'.csv').iloc[:,:].values\n",
    "                audio_features_mfcc_xbow = pd.read_csv('./AVEC2019-master/Baseline_systems/CES/output/mfcc_xbow/'+filename.split('.')[0]+'.csv').iloc[:,:].values\n",
    "                audio_features_mfcc_functionals = pd.read_csv('./AVEC2019-master/Baseline_systems/CES/output/mfcc_functionals/'+filename.split('.')[0]+'.csv').iloc[:,:].values\n",
    "                # audio_features_deepspectrum = (audio_features_deepspectrum-audio_features_deepspectrum.mean())/(audio_features_deepspectrum.max()-audio_features_deepspectrum.min())\n",
    "                # audio_features_egemaps_xbow = (audio_features_egemaps_xbow-audio_features_egemaps_xbow.mean())/(audio_features_egemaps_xbow.max()-audio_features_egemaps_xbow.min())\n",
    "                # audio_features_egemaps_functionals = (audio_features_egemaps_functionals - audio_features_egemaps_functionals.mean())/(audio_features_egemaps_functionals.max()-audio_features_egemaps_functionals.min())\n",
    "                # audio_features_mfcc_xbow = (audio_features_mfcc_xbow - audio_features_mfcc_xbow.mean())/(audio_features_mfcc_xbow.max()-audio_features_mfcc_xbow.min())\n",
    "                # audio_features_mfcc_functionals = (audio_features_mfcc_functionals- audio_features_mfcc_functionals.mean())/(audio_features_mfcc_functionals.max()-audio_features_mfcc_functionals.min())\n",
    "\n",
    "                min_shape = min(audio_features_deepspectrum.shape[0],audio_features_egemaps_xbow.shape[0],audio_features_egemaps_functionals.shape[0],audio_features_mfcc_xbow.shape[0],audio_features_mfcc_functionals.shape[0])\n",
    "                all_feat = np.concatenate([audio_features_deepspectrum[:min_shape,],audio_features_egemaps_xbow[:min_shape,],audio_features_egemaps_functionals[:min_shape,],audio_features_mfcc_xbow[:min_shape,],audio_features_mfcc_functionals[:min_shape,]],axis=1)\n",
    "                # all_feat = audio_features_deepspectrum[:min_shape,]+audio_features_egemaps_xbow[:min_shape,]+audio_features_egemaps_functionals[:min_shape,]+audio_features_mfcc_xbow[:min_shape,]+audio_features_mfcc_functionals[:min_shape,]\n",
    "                # all_feat = (all_feat)/(all_feat.max()-all_feat.min())\n",
    "                if all_feat.shape[0]>=max_length:\n",
    "                    all_feat = all_feat[:max_length,:]\n",
    "                else:\n",
    "                    all_feat = np.concatenate([all_feat,np.zeros((max_length-all_feat.shape[0],all_feat.shape[1]))],axis=0)\n",
    "                all_feat = torch.tensor(all_feat,dtype=torch.float32)\n",
    "                train_file_list.append(all_feat)\n",
    "                y = pd.read_csv('./AVEC2019_CES_traindevel/labels/'+filename.split('.')[0]+'.csv',sep=';')\n",
    "                y = torch.tensor(y[label].values,dtype=torch.float32)\n",
    "                train_y_list.append(y)\n",
    "            else:\n",
    "                audio_features_deepspectrum = pd.read_csv('./AVEC2019-master/Baseline_systems/CES/output/deepspectrum/'+filename.split('.')[0]+'.csv').iloc[:,:].values\n",
    "                audio_features_egemaps_xbow = pd.read_csv('./AVEC2019-master/Baseline_systems/CES/output/egemaps_xbow/'+filename.split('.')[0]+'.csv').iloc[:,:].values\n",
    "                audio_features_egemaps_functionals = pd.read_csv('./AVEC2019-master/Baseline_systems/CES/output/egemaps_functionals/'+filename.split('.')[0]+'.csv').iloc[:,:].values\n",
    "                audio_features_mfcc_xbow = pd.read_csv('./AVEC2019-master/Baseline_systems/CES/output/mfcc_xbow/'+filename.split('.')[0]+'.csv').iloc[:,:].values\n",
    "                audio_features_mfcc_functionals = pd.read_csv('./AVEC2019-master/Baseline_systems/CES/output/mfcc_functionals/'+filename.split('.')[0]+'.csv').iloc[:,:].values\n",
    "                # audio_features_deepspectrum = (audio_features_deepspectrum-audio_features_deepspectrum.mean())/(audio_features_deepspectrum.max()-audio_features_deepspectrum.min())\n",
    "                # audio_features_egemaps_xbow = (audio_features_egemaps_xbow-audio_features_egemaps_xbow.mean())/(audio_features_egemaps_xbow.max()-audio_features_egemaps_xbow.min())\n",
    "                # audio_features_egemaps_functionals = (audio_features_egemaps_functionals - audio_features_egemaps_functionals.mean())/(audio_features_egemaps_functionals.max()-audio_features_egemaps_functionals.min())\n",
    "                # audio_features_mfcc_xbow = (audio_features_mfcc_xbow - audio_features_mfcc_xbow.mean())/(audio_features_mfcc_xbow.max()-audio_features_mfcc_xbow.min())\n",
    "                # audio_features_mfcc_functionals = (audio_features_mfcc_functionals- audio_features_mfcc_functionals.mean())/(audio_features_mfcc_functionals.max()-audio_features_mfcc_functionals.min())\n",
    "\n",
    "                min_shape = min(audio_features_deepspectrum.shape[0],audio_features_egemaps_xbow.shape[0],audio_features_egemaps_functionals.shape[0],audio_features_mfcc_xbow.shape[0],audio_features_mfcc_functionals.shape[0])\n",
    "                all_feat = np.concatenate([audio_features_deepspectrum[:min_shape,],audio_features_egemaps_xbow[:min_shape,],audio_features_egemaps_functionals[:min_shape,],audio_features_mfcc_xbow[:min_shape,],audio_features_mfcc_functionals[:min_shape,]],axis=1)\n",
    "                # all_feat = audio_features_deepspectrum[:min_shape,]+audio_features_egemaps_xbow[:min_shape,]+audio_features_egemaps_functionals[:min_shape,]+audio_features_mfcc_xbow[:min_shape,]+audio_features_mfcc_functionals[:min_shape,]\n",
    "                # all_feat = (all_feat)/(all_feat.max()-all_feat.min())\n",
    "                if all_feat.shape[0]>=max_length:\n",
    "                    all_feat = all_feat[:max_length,:]\n",
    "                else:\n",
    "                    all_feat = np.concatenate([all_feat,np.zeros((max_length-all_feat.shape[0],all_feat.shape[1]))],axis=0)\n",
    "                all_feat = torch.tensor(all_feat,dtype=torch.float32)\n",
    "                dev_file_list.append(all_feat)\n",
    "                y = pd.read_csv('./AVEC2019_CES_traindevel/labels/'+filename.split('.')[0]+'.csv',sep=';')\n",
    "                y = torch.tensor(y[label].values,dtype=torch.float32)\n",
    "                dev_y_list.append(y)\n",
    "        else:\n",
    "            if Culture in filename:\n",
    "                if 'Train' in filename:\n",
    "                    audio_features_deepspectrum = pd.read_csv('./AVEC2019-master/Baseline_systems/CES/output/deepspectrum/'+filename.split('.')[0]+'.csv').iloc[:,:].values\n",
    "                    audio_features_egemaps_xbow = pd.read_csv('./AVEC2019-master/Baseline_systems/CES/output/egemaps_xbow/'+filename.split('.')[0]+'.csv').iloc[:,:].values\n",
    "                    audio_features_egemaps_functionals = pd.read_csv('./AVEC2019-master/Baseline_systems/CES/output/egemaps_functionals/'+filename.split('.')[0]+'.csv').iloc[:,:].values\n",
    "                    audio_features_mfcc_xbow = pd.read_csv('./AVEC2019-master/Baseline_systems/CES/output/mfcc_xbow/'+filename.split('.')[0]+'.csv').iloc[:,:].values\n",
    "                    audio_features_mfcc_functionals = pd.read_csv('./AVEC2019-master/Baseline_systems/CES/output/mfcc_functionals/'+filename.split('.')[0]+'.csv').iloc[:,:].values\n",
    "                    # audio_features_deepspectrum = (audio_features_deepspectrum-audio_features_deepspectrum.mean())/(audio_features_deepspectrum.max()-audio_features_deepspectrum.min())\n",
    "                    # audio_features_egemaps_xbow = (audio_features_egemaps_xbow-audio_features_egemaps_xbow.mean())/(audio_features_egemaps_xbow.max()-audio_features_egemaps_xbow.min())\n",
    "                    # audio_features_egemaps_functionals = (audio_features_egemaps_functionals - audio_features_egemaps_functionals.mean())/(audio_features_egemaps_functionals.max()-audio_features_egemaps_functionals.min())\n",
    "                    # audio_features_mfcc_xbow = (audio_features_mfcc_xbow - audio_features_mfcc_xbow.mean())/(audio_features_mfcc_xbow.max()-audio_features_mfcc_xbow.min())\n",
    "                    # audio_features_mfcc_functionals = (audio_features_mfcc_functionals- audio_features_mfcc_functionals.mean())/(audio_features_mfcc_functionals.max()-audio_features_mfcc_functionals.min())\n",
    "\n",
    "                    min_shape = min(audio_features_deepspectrum.shape[0],audio_features_egemaps_xbow.shape[0],audio_features_egemaps_functionals.shape[0],audio_features_mfcc_xbow.shape[0],audio_features_mfcc_functionals.shape[0])\n",
    "                    all_feat = np.concatenate([audio_features_deepspectrum[:min_shape,],audio_features_egemaps_xbow[:min_shape,],audio_features_egemaps_functionals[:min_shape,],audio_features_mfcc_xbow[:min_shape,],audio_features_mfcc_functionals[:min_shape,]],axis=1)\n",
    "                    # all_feat = audio_features_deepspectrum[:min_shape,]+audio_features_egemaps_xbow[:min_shape,]+audio_features_egemaps_functionals[:min_shape,]+audio_features_mfcc_xbow[:min_shape,]+audio_features_mfcc_functionals[:min_shape,]\n",
    "                    # all_feat = (all_feat)/(all_feat.max()-all_feat.min())\n",
    "                    if all_feat.shape[0]>=max_length:\n",
    "                        all_feat = all_feat[:max_length,:]\n",
    "                    else:\n",
    "                        all_feat = np.concatenate([all_feat,np.zeros((max_length-all_feat.shape[0],all_feat.shape[1]))],axis=0)\n",
    "                    all_feat = torch.tensor(all_feat,dtype=torch.float32)\n",
    "                    train_file_list.append(all_feat)\n",
    "                    y = pd.read_csv('./AVEC2019_CES_traindevel/labels/'+filename.split('.')[0]+'.csv',sep=';')\n",
    "                    y = torch.tensor(y[label].values,dtype=torch.float32)\n",
    "                    train_y_list.append(y)\n",
    "                else:\n",
    "                    audio_features_deepspectrum = pd.read_csv('./AVEC2019-master/Baseline_systems/CES/output/deepspectrum/'+filename.split('.')[0]+'.csv').iloc[:,:].values\n",
    "                    audio_features_egemaps_xbow = pd.read_csv('./AVEC2019-master/Baseline_systems/CES/output/egemaps_xbow/'+filename.split('.')[0]+'.csv').iloc[:,:].values\n",
    "                    audio_features_egemaps_functionals = pd.read_csv('./AVEC2019-master/Baseline_systems/CES/output/egemaps_functionals/'+filename.split('.')[0]+'.csv').iloc[:,:].values\n",
    "                    audio_features_mfcc_xbow = pd.read_csv('./AVEC2019-master/Baseline_systems/CES/output/mfcc_xbow/'+filename.split('.')[0]+'.csv').iloc[:,:].values\n",
    "                    audio_features_mfcc_functionals = pd.read_csv('./AVEC2019-master/Baseline_systems/CES/output/mfcc_functionals/'+filename.split('.')[0]+'.csv').iloc[:,:].values\n",
    "                    # audio_features_deepspectrum = (audio_features_deepspectrum-audio_features_deepspectrum.mean())/(audio_features_deepspectrum.max()-audio_features_deepspectrum.min())\n",
    "                    # audio_features_egemaps_xbow = (audio_features_egemaps_xbow-audio_features_egemaps_xbow.mean())/(audio_features_egemaps_xbow.max()-audio_features_egemaps_xbow.min())\n",
    "                    # audio_features_egemaps_functionals = (audio_features_egemaps_functionals - audio_features_egemaps_functionals.mean())/(audio_features_egemaps_functionals.max()-audio_features_egemaps_functionals.min())\n",
    "                    # audio_features_mfcc_xbow = (audio_features_mfcc_xbow - audio_features_mfcc_xbow.mean())/(audio_features_mfcc_xbow.max()-audio_features_mfcc_xbow.min())\n",
    "                    # audio_features_mfcc_functionals = (audio_features_mfcc_functionals- audio_features_mfcc_functionals.mean())/(audio_features_mfcc_functionals.max()-audio_features_mfcc_functionals.min())\n",
    "\n",
    "                    min_shape = min(audio_features_deepspectrum.shape[0],audio_features_egemaps_xbow.shape[0],audio_features_egemaps_functionals.shape[0],audio_features_mfcc_xbow.shape[0],audio_features_mfcc_functionals.shape[0])\n",
    "                    all_feat = np.concatenate([audio_features_deepspectrum[:min_shape,],audio_features_egemaps_xbow[:min_shape,],audio_features_egemaps_functionals[:min_shape,],audio_features_mfcc_xbow[:min_shape,],audio_features_mfcc_functionals[:min_shape,]],axis=1)\n",
    "                    # all_feat = audio_features_deepspectrum[:min_shape,]+audio_features_egemaps_xbow[:min_shape,]+audio_features_egemaps_functionals[:min_shape,]+audio_features_mfcc_xbow[:min_shape,]+audio_features_mfcc_functionals[:min_shape,]\n",
    "                    # all_feat = (all_feat)/(all_feat.max()-all_feat.min())\n",
    "                    if all_feat.shape[0]>=max_length:\n",
    "                        all_feat = all_feat[:max_length,:]\n",
    "                    else:\n",
    "                        all_feat = np.concatenate([all_feat,np.zeros((max_length-all_feat.shape[0],all_feat.shape[1]))],axis=0)\n",
    "                    all_feat = torch.tensor(all_feat,dtype=torch.float32)\n",
    "                    dev_file_list.append(all_feat)\n",
    "                    y = pd.read_csv('./AVEC2019_CES_traindevel/labels/'+filename.split('.')[0]+'.csv',sep=';')\n",
    "                    y = torch.tensor(y[label].values,dtype=torch.float32)\n",
    "                    dev_y_list.append(y)\n",
    "    data_process_end = time.time()\n",
    "    print(f'data_process_time:{str(data_process_end-data_process_begin)}')\n",
    "    emotion_model = MEFARG().to(device)\n",
    "    loss_fn = ConcordanceCC()\n",
    "    optimizer = torch.optim.RMSprop(emotion_model.parameters(),lr = learning_rate)\n",
    "    max_ccc = 0\n",
    "    max_ch_ccc = 0\n",
    "    max_br_ccc = 0\n",
    "    fold_id = []\n",
    "    epoch_id = []\n",
    "    train_loss = []\n",
    "    infer_loss = []\n",
    "    K_Fold = StratifiedKFold(n_splits=fold, shuffle=True, random_state=42)\n",
    "    CV_begin = time.time()\n",
    "    for fold, (train_index, val_index) in enumerate(K_Fold.split(train_file_list, [0 for i in range(len(train_file_list))])):\n",
    "        print(f'fold {str(fold)} begin')\n",
    "        CV_begin_one_fold = time.time()\n",
    "        for epoch in range(EPOCHS):\n",
    "            fold_id.append(fold)\n",
    "            epoch_id.append(epoch)\n",
    "            CV_begin_one_epoch_train = time.time()\n",
    "            emotion_model.train()\n",
    "            for all_feat,y in zip([j for i,j in enumerate(train_file_list) if i in train_index],[j for i,j in enumerate(train_y_list) if i in train_index]):\n",
    "                optimizer.zero_grad()\n",
    "                all_feat = torch.transpose(torch.reshape(all_feat,[all_feat.shape[0],32,5]),1,2)\n",
    "                y_pred,y_pred_5 = emotion_model(all_feat.to(device))\n",
    "                # y_pred = (y_pred-y_pred.mean())/(y_pred.max()-y_pred.min())\n",
    "\n",
    "                y_true = torch.unsqueeze(y,axis=0).to(device)\n",
    "                if y_true.shape[1] < max_length:\n",
    "                    y_true = torch.cat([y_true,torch.zeros((1,max_length-y_true.shape[1])).to(device)],axis=1)\n",
    "                else:\n",
    "                    y_true = y_true[:,:max_length]\n",
    "                cccloss = 2-loss_fn(torch.unsqueeze(y_pred.flatten(),axis=0),y_true)#-loss_fn(y_pred_5,torch.cat([y_true for i in range(5)],dim=0))\n",
    "                cccloss.backward()\n",
    "                optimizer.step()#参数更新\n",
    "            CV_end_one_epoch_train = time.time()\n",
    "            print(f'CV_one_epoch_train_time:{str(CV_end_one_epoch_train-CV_begin_one_epoch_train)}')\n",
    "            CV_begin_one_epoch_infer = time.time()\n",
    "            emotion_model.eval()\n",
    "            with torch.no_grad():\n",
    "                dev_cc_loss = 0.0\n",
    "                y_l = []\n",
    "                feat_y_l = []\n",
    "                for all_feat,y in zip(dev_file_list,dev_y_list):\n",
    "                    y_l += y.data.cpu().numpy().tolist()\n",
    "                    shape_y = y.shape[0]\n",
    "                    all_feat = torch.transpose(torch.reshape(all_feat,[all_feat.shape[0],32,5]),1,2)\n",
    "                    y_pred,y_pred_5 = emotion_model(all_feat.to(device))\n",
    "                    # y_pred = (y_pred-y_pred.mean())/(y_pred.max()-y_pred.min())\n",
    "                    # y_pred = torch.unsqueeze(y_pred_5.mean(0),dim=0)\n",
    "                    y_true = torch.unsqueeze(y,axis=0).to(device)\n",
    "                    if y_true.shape[1] < max_length:\n",
    "                        y_true = torch.cat([y_true,torch.zeros((1,max_length-y_true.shape[1])).to(device)],axis=1)\n",
    "                    else:\n",
    "                        y_true = y_true[:,:max_length]\n",
    "                    cccloss = loss_fn(torch.unsqueeze(y_pred.flatten(),axis=0),y_true)\n",
    "                    dev_cc_loss += cccloss.item()\n",
    "                    feat_y_l.append(y_pred_5[0,:shape_y,:].data.cpu().numpy())\n",
    "                feat_y = np.concatenate(feat_y_l,axis=0)\n",
    "                feat_df = pd.DataFrame()\n",
    "                feat_df[label] = y_l\n",
    "                for i in range(feat_y.shape[1]):\n",
    "                    feat_df['feat_'+str(i)] = feat_y[:,i]\n",
    "                ccc_loss = dev_cc_loss/len(dev_file_list)\n",
    "                print(f'dev_CCC_loss:{ccc_loss}')\n",
    "                if ccc_loss>max_ccc:\n",
    "                    max_ccc = ccc_loss\n",
    "                    torch.save(emotion_model.state_dict(), f'./saved_model_TTP/{Culture}_{label}_{ccc_loss}_model.bin')\n",
    "                    feat_df.to_csv(f'./saved_feat_TTP/{Culture}_{label}_{ccc_loss}.csv',index=0)\n",
    "            infer_loss.append(ccc_loss)\n",
    "            with torch.no_grad():\n",
    "                train_cc_loss = 0.0\n",
    "                y_l = []\n",
    "                for all_feat,y in zip(train_file_list,train_y_list):\n",
    "                    y_l += y.data.cpu().numpy().tolist()\n",
    "                    shape_y = y.shape[0]\n",
    "                    all_feat = torch.transpose(torch.reshape(all_feat,[all_feat.shape[0],32,5]),1,2)\n",
    "                    y_pred,y_pred_5 = emotion_model(all_feat.to(device))\n",
    "                    y_true = torch.unsqueeze(y,axis=0).to(device)\n",
    "                    if y_true.shape[1] < max_length:\n",
    "                        y_true = torch.cat([y_true,torch.zeros((1,max_length-y_true.shape[1])).to(device)],axis=1)\n",
    "                    else:\n",
    "                        y_true = y_true[:,:max_length]\n",
    "                    cccloss = loss_fn(torch.unsqueeze(y_pred.flatten(),axis=0),y_true)\n",
    "                    train_cc_loss += cccloss.item()\n",
    "                ccc_loss = train_cc_loss/len(train_file_list)\n",
    "            train_loss.append(ccc_loss)\n",
    "            CV_end_one_epoch_infer = time.time()\n",
    "            print(f'CV_one_epoch_infer_time:{str(CV_end_one_epoch_infer-CV_begin_one_epoch_infer)}')\n",
    "        CV_end_one_fold = time.time()\n",
    "        print(f'CV_one_fold_time:{str(CV_end_one_fold-CV_begin_one_fold)}')\n",
    "    cv_result = pd.DataFrame()\n",
    "    cv_result['fold_id'] = fold_id\n",
    "    cv_result['epoch_id'] = epoch_id\n",
    "    cv_result['train_loss'] = train_loss\n",
    "    cv_result['infer_loss'] = infer_loss\n",
    "    result.to_csv(f'./saved_cv_result/{Culture}_{label}_{ccc_loss}.csv',index=0)\n",
    "    CV_end = time.time()\n",
    "    print(f'CV_time:{str(CV_end-CV_begin)}')\n",
    "    print(f'{Culture}__{label}__ccc_loss:{max_ccc}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46df635f-05ea-4dbb-b139-69d59a687db3",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_AMEF(Culture='DE',label='valence',learning_rate=0.005)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10a9fe3e-35fa-4d3b-97e5-442bad18d1d4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0cf9c624-53a7-4cf0-8a62-c45dbda8346e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "transformer",
   "language": "python",
   "name": "transformer"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
